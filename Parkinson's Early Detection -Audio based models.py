# -*- coding: utf-8 -*-
"""IlabData2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pwGFlz_osKoVGYDq912Z-3OJ_VQPU0A2
"""

!pip -q install librosa==0.10.1 soundfile pandas numpy scipy scikit-learn tqdm

import numpy as np
import pandas as pd
from tqdm import tqdm
from scipy.stats import iqr
import librosa
from sklearn.feature_selection import mutual_info_classif
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
import os
import math
import os, glob, math, warnings
warnings.filterwarnings("ignore")

from google.colab import drive
drive.mount('/content/drive')

BASE_DIR = "/content/drive/MyDrive/Ilab"


HC_DIR = os.path.join(BASE_DIR, "HC_AH")
PD_DIR = os.path.join(BASE_DIR, "PD_AH")
DEMOGRAPHICS_CSV = os.path.join(BASE_DIR, "Demographics_age_sex.xlsx")
OUTPUT_CSV = os.path.join(BASE_DIR, "audio_features.csv")

print("HC_DIR:", HC_DIR)
print("PD_DIR:", PD_DIR)
print("DEMOGRAPHICS_CSV:", DEMOGRAPHICS_CSV)
print("OUTPUT_CSV:", OUTPUT_CSV)

#  Audio params
SR = 16000
FRAME_LEN = 0.025     # 25 ms
HOP_LEN   = 0.010     # 10 ms
N_FFT     = int(SR * FRAME_LEN)   # 400
HOP_S     = int(SR * HOP_LEN)     # 160

# Helpers
def list_audio(folder):
    exts = ("*.wav","*.mp3","*.flac","*.m4a")
    files=[]
    for e in exts:
        files += glob.glob(os.path.join(folder,"**",e), recursive=True)
    return sorted(files)

def load_wave(path, sr=SR):
    y, _ = librosa.load(path, sr=sr, mono=True)
    if y.size == 0:
        y = np.zeros(sr//2, dtype=float)

    y = librosa.util.normalize(y)
    return y, sr

def stats(v):
    v = np.asarray(v, dtype=float)
    if v.size == 0 or not np.isfinite(v).any():
        return dict(mean=0.0, std=0.0, med=0.0, min=0.0, max=0.0, iqr=0.0)
    return dict(
        mean=float(np.nanmean(v)),
        std =float(np.nanstd(v, ddof=1)) if v.size>1 else 0.0,
        med =float(np.nanmedian(v)),
        min =float(np.nanmin(v)),
        max =float(np.nanmax(v)),
        iqr =float(iqr(v)) if v.size>1 else 0.0,
    )

def add_stats_row(row, prefix, v):
    s = stats(v)
    for k, val in s.items():
        row[f"{prefix}_{k}"] = val

def extract_features_librosa(path):
    row = {"name": os.path.splitext(os.path.basename(path))[0]}

    try:
        row["duration_sec"] = float(librosa.get_duration(filename=path))
    except Exception:
        y_tmp, sr_tmp = load_wave(path, SR)
        row["duration_sec"] = float(len(y_tmp)/sr_tmp)

    # features
    y, sr = load_wave(path, SR)
    rms = librosa.feature.rms(y=y, frame_length=N_FFT, hop_length=HOP_S)[0]
    zcr = librosa.feature.zero_crossing_rate(y, frame_length=N_FFT, hop_length=HOP_S)[0]
    add_stats_row(row, "rms", rms)
    add_stats_row(row, "zcr", zcr)

    S = np.abs(librosa.stft(y, n_fft=N_FFT, hop_length=HOP_S)) + 1e-9
    add_stats_row(row, "spec_centroid",  librosa.feature.spectral_centroid(S=S, sr=sr)[0])
    add_stats_row(row, "spec_bandwidth", librosa.feature.spectral_bandwidth(S=S, sr=sr)[0])
    add_stats_row(row, "spec_rolloff95", librosa.feature.spectral_rolloff(S=S, sr=sr, roll_percent=0.95)[0])
    add_stats_row(row, "spec_flatness",  librosa.feature.spectral_flatness(S=S)[0])
    contrast = librosa.feature.spectral_contrast(S=S, sr=sr)
    add_stats_row(row, "spec_contrast",  np.mean(contrast, axis=0))

    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, n_fft=N_FFT, hop_length=HOP_S)
    d1   = librosa.feature.delta(mfcc, order=1)
    d2   = librosa.feature.delta(mfcc, order=2)
    for i in range(13):
        row[f"mfcc{i+1}_mean"]   = float(np.nanmean(mfcc[i]))
        row[f"mfcc{i+1}_std"]    = float(np.nanstd(mfcc[i], ddof=1)) if mfcc.shape[1]>1 else 0.0
        row[f"dmfcc{i+1}_mean"]  = float(np.nanmean(d1[i]))
        row[f"dmfcc{i+1}_std"]   = float(np.nanstd(d1[i], ddof=1)) if d1.shape[1]>1 else 0.0
        row[f"ddmfcc{i+1}_mean"] = float(np.nanmean(d2[i]))
        row[f"ddmfcc{i+1}_std"]  = float(np.nanstd(d2[i], ddof=1)) if d2.shape[1]>1 else 0.0

    try:
        f0, _, _ = librosa.pyin(y, fmin=50, fmax=600, sr=sr, frame_length=N_FFT, hop_length=HOP_S)
    except Exception:
        f0 = None
    if f0 is None or f0.size == 0:
        row.update({"f0_mean":0.0,"f0_std":0.0,"f0_med":0.0,"f0_min":0.0,"f0_max":0.0,"f0_range":0.0,"voiced_ratio":0.0})
    else:
        voiced = ~np.isnan(f0)
        f0v = f0[voiced]
        if f0v.size == 0:
            row.update({"f0_mean":0.0,"f0_std":0.0,"f0_med":0.0,"f0_min":0.0,"f0_max":0.0,"f0_range":0.0,
                        "voiced_ratio": float(np.mean(voiced)) if f0.size else 0.0})
        else:
            row["f0_mean"]   = float(np.mean(f0v))
            row["f0_std"]    = float(np.std(f0v, ddof=1)) if f0v.size>1 else 0.0
            row["f0_med"]    = float(np.median(f0v))
            row["f0_min"]    = float(np.min(f0v))
            row["f0_max"]    = float(np.max(f0v))
            row["f0_range"]  = float(np.max(f0v) - np.min(f0v))
            row["voiced_ratio"] = float(np.mean(voiced)) if f0.size else 0.0
    return row


hc_files = list_audio(HC_DIR)
pd_files = list_audio(PD_DIR)
print(f"Found {len(hc_files)} HC files, {len(pd_files)} PD files.")
assert len(hc_files)+len(pd_files) > 0, "No audio files found. Check HC_DIR/PD_DIR and file extensions."


rows=[]
for f in tqdm(hc_files, desc="HC"):
    r = extract_features_librosa(f); r["status"]=0; rows.append(r)
for f in tqdm(pd_files, desc="PD"):
    r = extract_features_librosa(f); r["status"]=1; rows.append(r)

df = pd.DataFrame(rows)
demographics_df = pd.read_excel(DEMOGRAPHICS_CSV)
df = df.merge(demographics_df[['Sample ID', 'Age', 'Sex']], how='left', left_on='name', right_on='Sample ID')



df.to_csv(OUTPUT_CSV, index=False)
print("Saved:", OUTPUT_CSV, "shape:", df.shape)

df = df.drop(columns=['Sample ID'])
print(df)

X = df.drop(columns=["name","status","Sex"]).apply(pd.to_numeric, errors="coerce")
X = X.fillna(X.median(numeric_only=True))
y = df["status"].astype(int)

print(df.shape)
print(df.info())
print(df.isna().sum())
print(df['status'].value_counts())

print(X.isna().sum())

# Replace 'M' with 1 (Male) and 'F' with 0 (Female)
df['Sex'] = df['Sex'].replace({'M': 1, 'F': 0})

# Check the first few rows to confirm the encoding
print(df[['Sex']].head())

# Step 1: Mutual Information ranking
mi = mutual_info_classif(X, y, random_state=42)
mi_scores = pd.Series(mi, index=X.columns).sort_values(ascending=False)

# Take top 70 first (shortlist)
top70 = mi_scores.head(70).index.tolist()
X_short = X[top70]

# Step 2: Correlation pruning → avoid redundant features
corr = X_short.corr().abs()
upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))
to_drop = [col for col in upper.columns if any(upper[col] > 0.90)]

X_pruned = X_short.drop(columns=to_drop)
print("After pruning, features left:", X_pruned.shape[1])

# Step 3: Final top 50 (if still >50, keep top MI-ranked)
final50 = mi_scores[mi_scores.index.isin(X_pruned.columns)].head(50).index.tolist()
data = df[["name","status","Sex"] + final50]


OUTPUT50 = OUTPUT_CSV.replace("BASELINE.csv", "TOP50.csv")
data.to_csv(OUTPUT50, index=False)
print("Saved:", OUTPUT50, "shape:", data.shape)


data.head()

data.describe()

# Identify features
feature_cols = [c for c in data.columns if c not in ["name","status"]]


plt.figure(figsize=(5,4))
sns.countplot(x="status", data=data, palette="Set2")
plt.title("Class Distribution (0 = Healthy, 1 = PD)")
plt.xlabel("Status")
plt.ylabel("Count")
plt.show()

plt.figure(figsize=(6,4))
sns.histplot(data["duration_sec"], kde=True, bins=20)
plt.title("Distribution of Audio Durations")
plt.xlabel("Duration (seconds)")
plt.ylabel("Count")
plt.show()

if "duration_sec" in df.columns:
    plt.figure(figsize=(6,4))
    sns.histplot(data=df, x="duration_sec", hue="status", bins=10, kde=True, palette="Set1")
    plt.title("Distribution of Audio Durations")
    plt.xlabel("Duration (seconds)")
    plt.ylabel("Count")
    plt.show()

    print("Average duration per class:")
    print(df.groupby("status")["duration_sec"].describe()[["mean","50%","min","max"]])

df.columns

plt.figure(figsize=(8, 6))
sns.kdeplot(data=df, x="Age", hue="status", fill=True, common_norm=False, palette="Set2")
plt.title("Age Distribution by Status (Healthy vs Parkinson's)")
plt.xlabel("Age")
plt.ylabel("Density")
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(8, 6))
sns.countplot(x="status", hue="Sex", data=df, palette="Set2")
plt.title("Count of Sex by Status (Healthy vs Parkinson's)")
plt.xlabel("Status")
plt.ylabel("Count")
plt.xticks([0, 1], ["Healthy", "Parkinson's"])
plt.legend(title="Sex", labels=["Female", "Male"])
plt.show()

n_features = len(feature_cols)


n_rows = math.ceil(n_features / 4)

plt.figure(figsize=(40, 4 * n_rows))

for i, f in enumerate(feature_cols, 1):
    plt.subplot(n_rows, 4, i)
    sns.kdeplot(data=data, x=f, hue="status", fill=True, common_norm=False, palette="Set1")
    plt.title(f, fontsize=10)
    plt.xlabel("")
    plt.ylabel("")

plt.tight_layout()
plt.show()

n_features = len(feature_cols)


n_rows = math.ceil(n_features / 4)

plt.figure(figsize=(20, 4 * n_rows))

for i, f in enumerate(feature_cols, 1):
    plt.subplot(n_rows, 4, i)
    sns.boxplot(x="status", y=f, data=data, palette="Set3", showfliers=False)
    plt.title(f, fontsize=10)
    plt.xlabel("")
    plt.ylabel("")

plt.tight_layout()
plt.show()

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=300, random_state=42)
rf.fit(data[feature_cols], data["status"])
importances = pd.Series(rf.feature_importances_, index=feature_cols).sort_values(ascending=True)

plt.figure(figsize=(8, 12))
importances.plot(kind="barh")
plt.title("Feature Importance (Random Forest)")
plt.show()

# === 4) Correlation Heatmap ===
plt.figure(figsize=(14,10))
sns.heatmap(data[feature_cols].corr(numeric_only=True), cmap="coolwarm", center=0, cbar=True)
plt.title("Feature Correlation Heatmap")
plt.show()

# orrelation of each feature with the target
corr_with_target = data[feature_cols + ["status"]].corr(numeric_only=True)["status"].drop("status")
corr_with_target = corr_with_target.sort_values(ascending=False)


plt.figure(figsize=(10, 14))
sns.barplot(x=corr_with_target.values, y=corr_with_target.index, palette="coolwarm")
plt.title("Correlation of Features with Target (status: 0=HC, 1=PD)")
plt.xlabel("Correlation with status")
plt.ylabel("Features")
plt.axvline(0, color="black", linestyle="--")
plt.show()

from sklearn.preprocessing import StandardScaler

X_scaled = StandardScaler().fit_transform(data[feature_cols].fillna(0))
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(6,5))
sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=data["status"], palette="Set2", alpha=0.8)
plt.title("PCA Scatterplot of Features (2D Projection)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.show()

"""# Logistic Regression"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, roc_auc_score, average_precision_score,
    confusion_matrix, classification_report, roc_curve, precision_recall_curve, ConfusionMatrixDisplay
)


META = ["name", "status"]
feature_cols = [c for c in data.columns if c not in META]

X = data[feature_cols].apply(pd.to_numeric, errors="coerce").fillna(data[feature_cols].median())
y = data["status"].astype(int)


X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.30, stratify=y, random_state=42
)
print(f"Shapes -> Train: {X_train.shape}, Test: {X_test.shape}")


model = Pipeline(steps=[
    ("scaler", StandardScaler()),
    ("lr", LogisticRegression(
        penalty="l2",
        solver="lbfgs",
        max_iter=1000,
        random_state=42,
        C=0.01
    ))
])


model.fit(X_train, y_train)


y_prob_train = model.predict_proba(X_train)[:, 1]
y_pred_train = (y_prob_train >= 0.5).astype(int)

print("\n=== Logistic Regression — TRAIN ===")
print(f"Accuracy: {accuracy_score(y_train, y_pred_train):.3f}")
print(f"ROC-AUC:  {roc_auc_score(y_train, y_prob_train):.3f}")
print(f"PR-AUC:   {average_precision_score(y_train, y_prob_train):.3f}")
print("Confusion matrix:\n", confusion_matrix(y_train, y_pred_train))
print(classification_report(y_train, y_pred_train, digits=3))


y_prob_test  = model.predict_proba(X_test)[:, 1]
y_pred_test  = (y_prob_test  >= 0.5).astype(int)

print("\n=== Logistic Regression — TEST ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred_test):.3f}")
print(f"ROC-AUC:  {roc_auc_score(y_test, y_prob_test):.3f}")
print(f"PR-AUC:   {average_precision_score(y_test, y_prob_test):.3f}")
print("Confusion matrix:\n", confusion_matrix(y_test, y_pred_test))
print(classification_report(y_test, y_pred_test, digits=3))


labels = [str(c) for c in np.unique(y)]

cm_train = confusion_matrix(y_train, y_pred_train, labels=np.unique(y))
ConfusionMatrixDisplay(cm_train, display_labels=labels).plot(cmap="Blues", values_format='d')
plt.title("Confusion Matrix — Train")
plt.tight_layout()
plt.show()


cm_test = confusion_matrix(y_test, y_pred_test, labels=np.unique(y))
ConfusionMatrixDisplay(cm_test, display_labels=labels).plot(cmap="Blues", values_format='d')
plt.title("Confusion Matrix — Test")
plt.tight_layout()
plt.show()

# ==== Curves on TEST ====
# ROC
fpr, tpr, _ = roc_curve(y_test, y_prob_test)
plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"AUC = {roc_auc_score(y_test, y_prob_test):.3f}")
plt.plot([0,1], [0,1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve — Test (Logistic Regression, C=0.01)")
plt.legend()
plt.tight_layout()
plt.show()

# Precision–Recall
prec, rec, _ = precision_recall_curve(y_test, y_prob_test)
plt.figure(figsize=(6,5))
plt.plot(rec, prec, label=f"AP = {average_precision_score(y_test, y_prob_test):.3f}")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision–Recall Curve — Test (Logistic Regression, C=0.01)")
plt.legend()
plt.tight_layout()
plt.show()

"""# Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, roc_auc_score, average_precision_score,
    confusion_matrix, classification_report, roc_curve, precision_recall_curve,
    ConfusionMatrixDisplay
)

# ====  Prepare X, y from your dataframe `data` ====
META = ["name", "status"]
feature_cols = [c for c in data.columns if c not in META]

X = data[feature_cols].apply(pd.to_numeric, errors="coerce").fillna(data[feature_cols].median())
y = data["status"].astype(int)

# ==== Train / Test split ====
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.30, stratify=y, random_state=42
)

# ==== Random Forest model (simple setup) ====
rf = RandomForestClassifier(
    n_estimators=500,
    max_depth=3,
    min_samples_leaf=15,
    min_samples_split=20,
    max_features="sqrt",
    random_state=42,
    n_jobs=-1
)
rf.fit(X_train, y_train)

# ====  Evaluate ====
# Train
y_prob_train = rf.predict_proba(X_train)[:, 1]
y_pred_train = (y_prob_train >= 0.5).astype(int)

print("\n=== Random Forest — TRAIN ===")
print(f"Accuracy: {accuracy_score(y_train, y_pred_train):.3f}")
print(f"ROC-AUC:  {roc_auc_score(y_train, y_prob_train):.3f}")
print(f"PR-AUC:   {average_precision_score(y_train, y_prob_train):.3f}")
print(classification_report(y_train, y_pred_train, digits=3))

# Test
y_prob_test = rf.predict_proba(X_test)[:, 1]
y_pred_test = (y_prob_test >= 0.5).astype(int)

print("\n=== Random Forest — TEST ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred_test):.3f}")
print(f"ROC-AUC:  {roc_auc_score(y_test, y_prob_test):.3f}")
print(f"PR-AUC:   {average_precision_score(y_test, y_prob_test):.3f}")

print(classification_report(y_test, y_pred_test, digits=3))
labels = [str(c) for c in np.unique(y)]


cm_train = confusion_matrix(y_train, y_pred_train, labels=np.unique(y))
ConfusionMatrixDisplay(cm_train, display_labels=labels).plot(cmap="Blues", values_format='d')
plt.title("Confusion Matrix — Train (Random Forest)")
plt.tight_layout()
plt.show()


cm_test = confusion_matrix(y_test, y_pred_test, labels=np.unique(y))
ConfusionMatrixDisplay(cm_test, display_labels=labels).plot(cmap="Blues", values_format='d')
plt.title("Confusion Matrix — Test (Random Forest)")
plt.tight_layout()
plt.show()


fpr, tpr, _ = roc_curve(y_test, y_prob_test)
plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"AUC = {roc_auc_score(y_test, y_prob_test):.3f}")
plt.plot([0,1], [0,1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve — Test (Random Forest)")
plt.legend()
plt.tight_layout()
plt.show()

prec, rec, _ = precision_recall_curve(y_test, y_prob_test)
plt.figure(figsize=(6,5))
plt.plot(rec, prec, label=f"AP = {average_precision_score(y_test, y_prob_test):.3f}")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision–Recall Curve — Test (Random Forest)")
plt.legend()
plt.tight_layout()
plt.show()

"""# Support Vector Machine"""

from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score, roc_auc_score, average_precision_score,
    confusion_matrix, classification_report, roc_curve, precision_recall_curve,
    ConfusionMatrixDisplay
)


META = ["name", "status"]
feature_cols = [c for c in data.columns if c not in META]

X = data[feature_cols].apply(pd.to_numeric, errors="coerce").fillna(data[feature_cols].median())
y = data["status"].astype(int)

print("X shape:", X.shape, "| class counts:", np.bincount(y))

# ====  Train / Test split ====
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.30, stratify=y, random_state=42
)


svm_model = Pipeline(steps=[
    ("scaler", StandardScaler()),
    ("svc", SVC(
        kernel="rbf",
        C=1,
        gamma=0.01,
        probability=True,
        random_state=42
    ))
])

# ==== Fit and evaluate ====
svm_model.fit(X_train, y_train)

# TRAIN metrics
y_prob_train = svm_model.predict_proba(X_train)[:, 1]
y_pred_train = (y_prob_train >= 0.5).astype(int)

print("\n=== SVM (RBF) — TRAIN ===")
print(f"Accuracy: {accuracy_score(y_train, y_pred_train):.3f}")
print(f"ROC-AUC:  {roc_auc_score(y_train, y_prob_train):.3f}")
print(f"PR-AUC:   {average_precision_score(y_train, y_prob_train):.3f}")
print(classification_report(y_train, y_pred_train, digits=3))

# TEST metrics
y_prob_test = svm_model.predict_proba(X_test)[:, 1]
y_pred_test = (y_prob_test >= 0.5).astype(int)

print("\n=== SVM (RBF) — TEST ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred_test):.3f}")
print(f"ROC-AUC:  {roc_auc_score(y_test, y_prob_test):.3f}")
print(f"PR-AUC:   {average_precision_score(y_test, y_prob_test):.3f}")
print(classification_report(y_test, y_pred_test, digits=3))


labels = [str(c) for c in np.unique(y)]

# TRAIN CM
cm_train = confusion_matrix(y_train, y_pred_train, labels=np.unique(y))
ConfusionMatrixDisplay(cm_train, display_labels=labels).plot(cmap="Blues", values_format='d')
plt.title("Confusion Matrix — Train (SVM RBF)")
plt.tight_layout()
plt.show()

# TEST CM
cm_test = confusion_matrix(y_test, y_pred_test, labels=np.unique(y))
ConfusionMatrixDisplay(cm_test, display_labels=labels).plot(cmap="Blues", values_format='d')
plt.title("Confusion Matrix — Test (SVM RBF)")
plt.tight_layout()
plt.show()

# ==== Curves on TEST ====
# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_prob_test)
plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"AUC = {roc_auc_score(y_test, y_prob_test):.3f}")
plt.plot([0,1], [0,1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve — Test (SVM RBF)")
plt.legend()
plt.tight_layout()
plt.show()

# Precision–Recall Curve
prec, rec, _ = precision_recall_curve(y_test, y_prob_test)
plt.figure(figsize=(6,5))
plt.plot(rec, prec, label=f"AP = {average_precision_score(y_test, y_prob_test):.3f}")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision–Recall Curve — Test (SVM RBF)")
plt.legend()
plt.tight_layout()
plt.show()

"""# TabPFN(v2)"""

!pip install tabpfn

try:
    from tabpfn import TabPFNClassifierV2 as TabPFNCls
    TABPFN_NAME = "TabPFNClassifierV2"
except Exception:
    from tabpfn import TabPFNClassifier as TabPFNCls
    TABPFN_NAME = "TabPFNClassifier"

RANDOM_STATE = 42

META = ["name", "status"]
assert "status" in data.columns, "Target column 'status' not found."

feature_cols = [c for c in data.columns if c not in META]
X_all = data[feature_cols].apply(pd.to_numeric, errors="coerce")
y_all = data["status"].astype(int).values

print(f"[INFO] Using {TABPFN_NAME}")
print("X shape:", X_all.shape, "| class counts:", np.bincount(y_all))

# ====  Train / Test split (before any imputation) ====
X_train_df, X_test_df, y_train, y_test = train_test_split(
    X_all, y_all, test_size=0.30, stratify=y_all, random_state=RANDOM_STATE
)


train_median = X_train_df.median(numeric_only=True)
X_train_df = X_train_df.fillna(train_median)
X_test_df  = X_test_df.fillna(train_median)


keep_cols = X_train_df.columns[X_train_df.nunique(dropna=False) > 1]
X_train_df = X_train_df[keep_cols]
X_test_df  = X_test_df[keep_cols]

X_train = X_train_df.values.astype(np.float32)
X_test  = X_test_df.values.astype(np.float32)

# ====  Define a simple TabPFN model ====

tabpfn_model = TabPFNCls(
    n_estimators=4,
    random_state=RANDOM_STATE
)

# ==== Fit and evaluate ====
tabpfn_model.fit(X_train, y_train)

# TRAIN metrics (use 0.5 threshold to match your SVM example)
y_prob_train = tabpfn_model.predict_proba(X_train)[:, 1]
y_pred_train = (y_prob_train >= 0.5).astype(int)

print("\n=== TabPFN — TRAIN ===")
print(f"Accuracy: {accuracy_score(y_train, y_pred_train):.3f}")
print(f"ROC-AUC:  {roc_auc_score(y_train, y_prob_train):.3f}")
print(f"PR-AUC:   {average_precision_score(y_train, y_prob_train):.3f}")
print(classification_report(y_train, y_pred_train, digits=3))

# TEST metrics
y_prob_test = tabpfn_model.predict_proba(X_test)[:, 1]
y_pred_test = (y_prob_test >= 0.5).astype(int)

print("\n=== TabPFN — TEST ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred_test):.3f}")
print(f"ROC-AUC:  {roc_auc_score(y_test, y_prob_test):.3f}")
print(f"PR-AUC:   {average_precision_score(y_test, y_prob_test):.3f}")
print(classification_report(y_test, y_pred_test, digits=3))

labels = [str(c) for c in np.unique(y_all)]

# TRAIN CM
cm_train = confusion_matrix(y_train, y_pred_train, labels=np.unique(y_all))
ConfusionMatrixDisplay(cm_train, display_labels=labels).plot(cmap="Blues", values_format='d')
plt.title("Confusion Matrix — Train (TabPFN)")
plt.tight_layout()
plt.show()

# TEST CM
cm_test = confusion_matrix(y_test, y_pred_test, labels=np.unique(y_all))
ConfusionMatrixDisplay(cm_test, display_labels=labels).plot(cmap="Blues", values_format='d')
plt.title("Confusion Matrix — Test (TabPFN)")
plt.tight_layout()
plt.show()


# ==== Curves on TEST ====
# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_prob_test)
plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"AUC = {roc_auc_score(y_test, y_prob_test):.3f}")
plt.plot([0,1], [0,1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve — Test (TabPFN)")
plt.legend()
plt.tight_layout()
plt.show()

# Precision–Recall Curve
prec, rec, _ = precision_recall_curve(y_test, y_prob_test)
plt.figure(figsize=(6,5))
plt.plot(rec, prec, label=f"AP = {average_precision_score(y_test, y_prob_test):.3f}")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision–Recall Curve — Test (TabPFN)")
plt.legend()
plt.tight_layout()
plt.show()

"""# Hypertab"""

pip install hypertab

import inspect, torch, random
from hypertab import HyperTabClassifier

SEED = 42
random.seed(SEED); np.random.seed(SEED)
try:
    torch.manual_seed(SEED)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(SEED)
except Exception:
    pass


META = ["name", "status"]
assert "status" in data.columns, "Target column 'status' not found."

feature_cols = [c for c in data.columns if c not in META]
X_all = data[feature_cols].apply(pd.to_numeric, errors="coerce")
y_all = data["status"].astype(int).values

print("X shape:", X_all.shape, "| class counts:", np.bincount(y_all))


X_train_df, X_test_df, y_train, y_test = train_test_split(
    X_all, y_all, test_size=0.30, stratify=y_all, random_state=SEED
)


train_median = X_train_df.median(numeric_only=True)
X_train_df = X_train_df.fillna(train_median)
X_test_df  = X_test_df.fillna(train_median)

keep_cols = X_train_df.columns[X_train_df.nunique(dropna=False) > 1]
X_train_df = X_train_df[keep_cols]
X_test_df  = X_test_df[keep_cols]

X_train = X_train_df.values.astype(np.float32)
X_test  = X_test_df.values.astype(np.float32)


def make_hypertab(**cfg):
    """Pass only kwargs supported by your installed HyperTabClassifier."""
    allowed = set(inspect.signature(HyperTabClassifier.__init__).parameters.keys())
    filtered = {k: v for k, v in cfg.items() if k in allowed and v is not None}
    return HyperTabClassifier(**filtered)

device = "cuda:0" if torch.cuda.is_available() else "cpu"


ht_model = make_hypertab(
    subsample=0.5,
    test_nodes=200,
    epochs=15,
    hidden_dims=16,
    device=device,
    seed=SEED,
    random_state=SEED
)

# ==== 4) Fit and evaluate ====
ht_model.fit(X_train, y_train)

# TRAIN metrics
y_prob_train = ht_model.predict_proba(X_train)[:, 1]
y_pred_train = (y_prob_train >= 0.5).astype(int)

print("\n=== HyperTab — TRAIN ===")
print(f"Accuracy: {accuracy_score(y_train, y_pred_train):.3f}")
print(f"ROC-AUC:  {roc_auc_score(y_train, y_prob_train):.3f}")
print(f"PR-AUC:   {average_precision_score(y_train, y_prob_train):.3f}")
print(classification_report(y_train, y_pred_train, digits=3))

# TEST
y_prob_test = ht_model.predict_proba(X_test)[:, 1]
y_pred_test = (y_prob_test >= 0.5).astype(int)

print("\n=== HyperTab — TEST ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred_test):.3f}")
print(f"ROC-AUC:  {roc_auc_score(y_test, y_prob_test):.3f}")
print(f"PR-AUC:   {average_precision_score(y_test, y_prob_test):.3f}")
print(classification_report(y_test, y_pred_test, digits=3))
labels = [str(c) for c in np.unique(y_all)]

# TRAIN CM
cm_train = confusion_matrix(y_train, y_pred_train, labels=np.unique(y_all))
ConfusionMatrixDisplay(cm_train, display_labels=labels).plot(cmap="Blues", values_format='d')
plt.title("Confusion Matrix — Train (HyperTab)")
plt.tight_layout()
plt.show()

# TEST CM
cm_test = confusion_matrix(y_test, y_pred_test, labels=np.unique(y_all))
ConfusionMatrixDisplay(cm_test, display_labels=labels).plot(cmap="Blues", values_format='d')
plt.title("Confusion Matrix — Test (HyperTab)")
plt.tight_layout()
plt.show()

fpr, tpr, _ = roc_curve(y_test, y_prob_test)
plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"AUC = {roc_auc_score(y_test, y_prob_test):.3f}")
plt.plot([0,1], [0,1], 'k--'); plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
plt.title("ROC Curve — Test (HyperTab)"); plt.legend(); plt.tight_layout(); plt.show()

prec, rec, _ = precision_recall_curve(y_test, y_prob_test)
plt.figure(figsize=(6,5))
plt.plot(rec, prec, label=f"AP = {average_precision_score(y_test, y_prob_test):.3f}")
plt.xlabel("Recall"); plt.ylabel("Precision")
plt.title("Precision–Recall Curve — Test (HyperTab)")
plt.legend(); plt.tight_layout(); plt.show()

"""# TABNET"""

pip install pytorch-tabnet

import numpy as np, pandas as pd, matplotlib.pyplot as plt, torch
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score, f1_score, confusion_matrix, classification_report, roc_curve, precision_recall_curve
from sklearn.feature_selection import mutual_info_classif
from pytorch_tabnet.tab_model import TabNetClassifier

SEED = 42
META = ["name","status"]

assert "status" in data.columns
feat = [c for c in data.columns if c not in META]
X_all = data[feat].apply(pd.to_numeric, errors="coerce")
y_all = data["status"].astype(int).values
print("Shape:", X_all.shape, "| class counts:", np.bincount(y_all))
X_tr_df, X_te_df, y_tr, y_te = train_test_split(X_all, y_all, test_size=0.30, stratify=y_all, random_state=SEED)
med = X_tr_df.median(numeric_only=True)
X_tr_df = X_tr_df.fillna(med); X_te_df = X_te_df.fillna(med)
keep = X_tr_df.columns[X_tr_df.nunique(dropna=False) > 1]
X_tr_df = X_tr_df[keep]; X_te_df = X_te_df[keep]


X_tr_in_df, X_val_df, y_tr_in, y_val = train_test_split(X_tr_df, y_tr, test_size=0.20, stratify=y_tr, random_state=SEED)
K = min(20, X_tr_in_df.shape[1])
mi = mutual_info_classif(X_tr_in_df.values, y_tr_in, random_state=SEED)
rank = pd.Series(mi, index=X_tr_in_df.columns).sort_values(ascending=False)
topk = rank.index[:K].tolist()
X_tr_in = X_tr_in_df[topk].values.astype(np.float32)
X_val   = X_val_df[topk].values.astype(np.float32)
X_tr    = X_tr_df[topk].values.astype(np.float32)
X_te    = X_te_df[topk].values.astype(np.float32)


device_name = "cuda" if torch.cuda.is_available() else "cpu"
tabnet = TabNetClassifier(
    n_d=8, n_a=8,
    n_steps=3,
    gamma=1.5,
    n_independent=1, n_shared=1,
    lambda_sparse=1e-3,
    optimizer_fn=torch.optim.AdamW,
    optimizer_params=dict(lr=3e-4, weight_decay=2e-4),
    scheduler_fn=torch.optim.lr_scheduler.StepLR,
    scheduler_params=dict(step_size=50, gamma=0.9),
    mask_type="entmax",
    verbose=0,
    device_name=device_name,
    seed=SEED
)


tabnet.fit(
    X_tr_in, y_tr_in,
    eval_set=[(X_val, y_val)],
    eval_name=["val"],
    eval_metric=["auc"],
    max_epochs=1000,
    patience=80,
    batch_size=32,
    virtual_batch_size=16
)

def best_thresh_by_f1(y_true, y_prob):
    pr, rc, th = precision_recall_curve(y_true, y_prob)
    cand = np.r_[0.0, th, 1.0]
    f1s  = [f1_score(y_true, (y_prob>=t).astype(int), zero_division=0) for t in cand]
    return float(cand[int(np.argmax(f1s))])

p_val = tabnet.predict_proba(X_val)[:,1]
th = best_thresh_by_f1(y_val, p_val)


tabnet.fit(
    X_tr, y_tr,
    eval_set=[(X_tr, y_tr)],
    eval_name=["train"],
    eval_metric=["auc"],
    max_epochs=tabnet.max_epochs,
    patience=tabnet.patience,
    batch_size=32,
    virtual_batch_size=16
)


p_tr = tabnet.predict_proba(X_tr)[:,1]; yhat_tr = (p_tr >= th).astype(int)
print("\n=== TabNet — TRAIN (refit; threshold from VAL) ===")
print(f"Accuracy: {accuracy_score(y_tr, yhat_tr):.3f}")
print(f"ROC-AUC:  {roc_auc_score(y_tr, p_tr):.3f}")
print(f"PR-AUC:   {average_precision_score(y_tr, p_tr):.3f}")
print(classification_report(y_tr, yhat_tr, digits=3))

p_te = tabnet.predict_proba(X_te)[:,1]; yhat_te = (p_te >= th).astype(int)
print("\n=== TabNet — TEST ===")
print(f"Accuracy: {accuracy_score(y_te, yhat_te):.3f}")
print(f"ROC-AUC:  {roc_auc_score(y_te, p_te):.3f}")
print(f"PR-AUC:   {average_precision_score(y_te, p_te):.3f}")
print(classification_report(y_te, yhat_te, digits=3))



labels = [str(c) for c in np.unique(y_all)]
cm_tr = confusion_matrix(y_tr, yhat_tr, labels=np.unique(y_all))
ConfusionMatrixDisplay(cm_tr, display_labels=labels).plot(cmap="Blues", values_format='d')
plt.title("Confusion Matrix — Train (TabNet)")
plt.tight_layout()
plt.show()


cm_te = confusion_matrix(y_te, yhat_te, labels=np.unique(y_all))
ConfusionMatrixDisplay(cm_te, display_labels=labels).plot(cmap="Blues", values_format='d')
plt.title("Confusion Matrix — Test (TabNet)")
plt.tight_layout()
plt.show()


fpr,tpr,_ = roc_curve(y_te, p_te)
plt.figure(figsize=(6,5)); plt.plot(fpr,tpr,label=f"AUC={roc_auc_score(y_te,p_te):.3f}")
plt.plot([0,1],[0,1],'k--'); plt.xlabel("FPR"); plt.ylabel("TPR"); plt.title("ROC — TabNet (small-N)"); plt.legend(); plt.tight_layout(); plt.show()

prec,rec,_ = precision_recall_curve(y_te,p_te)
plt.figure(figsize=(6,5)); plt.plot(rec,prec,label=f"AP={average_precision_score(y_te,p_te):.3f}")
plt.xlabel("Recall"); plt.ylabel("Precision"); plt.title("PR — TabNet (small-N)"); plt.legend(); plt.tight_layout(); plt.show()

"""# Saint Lite Model

"""

import torch.nn as nn
SEED = 42
torch.manual_seed(SEED); np.random.seed(SEED)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


D_MODEL   = 24
N_HEADS   = 4
N_LAYERS  = 1
DROPOUT   = 0.30
TOK_DROPOUT = 0.30
EPOCHS    = 250
BATCHSIZE = 32
LR        = 6e-4
WEIGHT_DECAY = 1e-3


META = ["name","status"]
assert "status" in data.columns, "Target column 'status' is missing."

feature_cols = [c for c in data.columns if c not in META]
X_all = data[feature_cols].apply(pd.to_numeric, errors="coerce")
y_all = data["status"].astype(int).values

print("X shape:", X_all.shape, "| class counts:", np.bincount(y_all))


X_train_df, X_test_df, y_train, y_test = train_test_split(
    X_all, y_all, test_size=0.30, stratify=y_all, random_state=SEED
)


med = X_train_df.median(numeric_only=True)
X_train_df = X_train_df.fillna(med)
X_test_df  = X_test_df.fillna(med)

keep = X_train_df.columns[X_train_df.nunique(dropna=False) > 1]
X_train_df = X_train_df[keep]
X_test_df  = X_test_df[keep]


scaler = StandardScaler().fit(X_train_df.values)
X_train = scaler.transform(X_train_df.values).astype(np.float32)
X_test  = scaler.transform(X_test_df.values).astype(np.float32)


class SAINTLite(nn.Module):
    """
    Each feature -> token: proj(value) + column embedding.
    Prepend a CLS token; pass through TransformerEncoder; classify from CLS.
    """
    def __init__(self, n_features, d_model=24, n_heads=4, n_layers=1, p_drop=0.30, p_tok=0.30):
        super().__init__()
        self.scalar_proj = nn.Linear(1, d_model)
        self.col_embed   = nn.Parameter(torch.randn(n_features, d_model) * 0.02)
        self.cls_token   = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)
        self.token_dropout = nn.Dropout(p_tok)

        enc_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=n_heads, dim_feedforward=d_model*4,
            dropout=p_drop, batch_first=True, activation="gelu", norm_first=True
        )
        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)
        self.norm = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, 1)

    def forward(self, x):        # x: (B, F)
        B, F = x.shape
        tok = self.scalar_proj(x.view(B, F, 1)) + self.col_embed.unsqueeze(0)  # (B,F,d)
        tok = self.token_dropout(tok)
        seq = torch.cat([self.cls_token.expand(B, -1, -1), tok], dim=1)        # (B,1+F,d)
        enc = self.encoder(seq)                                                # (B,1+F,d)
        cls = self.norm(enc[:, 0, :])                                          # (B,d)
        return self.head(cls).squeeze(-1)                                      # (B,)

    def predict_proba(self, x):
        self.eval()
        with torch.no_grad():
            return torch.sigmoid(self.forward(x))


n_features = X_train.shape[1]
model = SAINTLite(
    n_features=n_features,
    d_model=D_MODEL, n_heads=N_HEADS, n_layers=N_LAYERS,
    p_drop=DROPOUT, p_tok=TOK_DROPOUT
).to(device)

opt  = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
crit = nn.BCEWithLogitsLoss()

Xtr_t = torch.from_numpy(X_train).to(device)
ytr_t = torch.from_numpy(y_train.astype(np.float32)).to(device)


for _ in range(EPOCHS):
    model.train()
    idx = np.random.permutation(len(X_train))
    for i in range(0, len(idx), BATCHSIZE):
        j = idx[i:i+BATCHSIZE]
        xb = Xtr_t[j]; yb = ytr_t[j]
        opt.zero_grad()
        loss = crit(model(xb), yb)
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), 2.0)
        opt.step()


def eval_block(X, y, title):
    Xt = torch.from_numpy(X).to(device)
    p  = model.predict_proba(Xt).cpu().numpy()
    yhat = (p >= 0.5).astype(int)
    print(f"\n=== SAINT — {title} ===")
    print(f"Accuracy: {accuracy_score(y, yhat):.3f}")
    print(f"ROC-AUC:  {roc_auc_score(y, p):.3f}")
    print(f"PR-AUC:   {average_precision_score(y, p):.3f}")
    print(classification_report(y, yhat, digits=3))
    return p

p_train = eval_block(X_train, y_train, "TRAIN")
p_test  = eval_block(X_test, y_test, "TEST")


labels = [str(c) for c in np.unique(np.r_[y_train, y_test])]

# TRAIN
cm_tr = confusion_matrix(y_train, (p_train >= 0.5).astype(int), labels=np.unique(np.r_[y_train, y_test]))
ConfusionMatrixDisplay(cm_tr, display_labels=labels).plot(cmap="Blues", values_format='d')
plt.title("Confusion Matrix — Train (SAINT)")
plt.tight_layout()
plt.show()

# TEST
cm_te = confusion_matrix(y_test, (p_test >= 0.5).astype(int), labels=np.unique(np.r_[y_train, y_test]))
ConfusionMatrixDisplay(cm_te, display_labels=labels).plot(cmap="Blues", values_format='d')
plt.title("Confusion Matrix — Test (SAINT)")
plt.tight_layout()
plt.show()


fpr, tpr, _ = roc_curve(y_test, p_test)
plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"AUC = {roc_auc_score(y_test, p_test):.3f}")
plt.plot([0,1],[0,1],'k--'); plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
plt.title("ROC Curve — Test (SAINT)"); plt.legend(); plt.tight_layout(); plt.show()

prec, rec, _ = precision_recall_curve(y_test, p_test)
plt.figure(figsize=(6,5))
plt.plot(rec, prec, label=f"AP = {average_precision_score(y_test, p_test):.3f}")
plt.xlabel("Recall"); plt.ylabel("Precision")
plt.title("Precision–Recall Curve — Test (SAINT)")
plt.legend(); plt.tight_layout(); plt.show()

"""# SVM Model"""

from joblib import dump
from datetime import datetime

svm_bundle = {
    "pipeline": svm_model,
    "feature_names": feature_cols,
    "threshold": 0.5,
    "created_at": datetime.utcnow().isoformat(timespec="seconds") + "Z"
}

dump(svm_bundle, "svm_pipeline.joblib")
print("Saved SVM pipeline -> svm_pipeline.joblib")

from joblib import load
import pandas as pd
import numpy as np

svm_bundle = load("svm_pipeline.joblib")
svm_pipe   = svm_bundle["pipeline"]
feat_order = svm_bundle["feature_names"]
thr        = svm_bundle["threshold"]

def svm_predict_proba(df: pd.DataFrame) -> np.ndarray:
    X = df[feat_order].apply(pd.to_numeric, errors="coerce")
    X = X.fillna(X.median(numeric_only=True))   # same simple median policy
    return svm_pipe.predict_proba(X)[:, 1]

def svm_predict(df: pd.DataFrame) -> np.ndarray:
    p = svm_predict_proba(df)
    return (p >= thr).astype(int)

"""# SAINT-lite Model"""

# ==== SAVE SAINT PREPROCESSING ====
from joblib import dump
from datetime import datetime


saint_preproc = {
    "medians": med.to_dict(),
    "keep_cols": list(keep),
    "feature_order": list(keep),
    "scaler": scaler,
    "threshold": 0.5,
    "class_names": ["0", "1"],
    "created_at": datetime.utcnow().isoformat(timespec="seconds") + "Z"
}
dump(saint_preproc, "saint_preproc.joblib")
print("Saved SAINT preprocessing -> saint_preproc.joblib")


import torch

saint_ckpt = {
    "state_dict": model.state_dict(),
    "model_config": {
        "n_features": n_features,
        "d_model": D_MODEL,
        "n_heads": N_HEADS,
        "n_layers": N_LAYERS,
        "p_drop": DROPOUT,
        "p_tok": TOK_DROPOUT
    }
}
torch.save(saint_ckpt, "saint_model.pt")
print("Saved SAINT weights -> saint_model.pt")

# ==== LOAD & PREDICT — SAINT ====
import torch, torch.nn as nn
import numpy as np, pandas as pd
from joblib import load


class SAINTLite(nn.Module):
    def __init__(self, n_features, d_model=24, n_heads=4, n_layers=1, p_drop=0.30, p_tok=0.30):
        super().__init__()
        self.scalar_proj = nn.Linear(1, d_model)
        self.col_embed   = nn.Parameter(torch.randn(n_features, d_model) * 0.02)
        self.cls_token   = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)
        self.token_dropout = nn.Dropout(p_tok)
        enc_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=n_heads, dim_feedforward=d_model*4,
            dropout=p_drop, batch_first=True, activation="gelu", norm_first=True
        )
        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)
        self.norm = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, 1)
    def forward(self, x):
        B, F = x.shape
        tok = self.scalar_proj(x.view(B, F, 1)) + self.col_embed.unsqueeze(0)
        tok = self.token_dropout(tok)
        seq = torch.cat([self.cls_token.expand(B, -1, -1), tok], dim=1)
        enc = self.encoder(seq)
        cls = self.norm(enc[:, 0, :])
        return self.head(cls).squeeze(-1)
    def predict_proba(self, x):
        self.eval()
        with torch.no_grad():
            return torch.sigmoid(self.forward(x))


device = torch.device("cpu")
pre = load("saint_preproc.joblib")
ckpt = torch.load("saint_model.pt", map_location=device)

model = SAINTLite(**ckpt["model_config"]).to(device)
model.load_state_dict(ckpt["state_dict"])
model.eval()

feature_order = pre["feature_order"]
medians = pre["medians"]
scaler  = pre["scaler"]
thr     = pre["threshold"]

def saint_prepare_X(df: pd.DataFrame) -> np.ndarray:
    X = df[feature_order].apply(pd.to_numeric, errors="coerce")
    X = X.fillna(pd.Series(medians))
    Xs = scaler.transform(X.values).astype(np.float32)
    return Xs

def saint_predict_proba(df: pd.DataFrame) -> np.ndarray:
    Xs = saint_prepare_X(df)
    with torch.no_grad():
        probs = model.predict_proba(torch.from_numpy(Xs).to(device)).cpu().numpy()
    return probs

def saint_predict(df: pd.DataFrame) -> np.ndarray:
    p = saint_predict_proba(df)
    return (p >= thr).astype(int)